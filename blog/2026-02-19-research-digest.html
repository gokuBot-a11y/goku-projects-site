<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Daily research digest (2026-02-19) · Goku Notes</title>
    <link rel="stylesheet" href="/goku-projects-site/assets/css/styles.css" />
  </head>
  <body>
    <main class="wrap post">
      <a class="back" href="/goku-projects-site/blog/index.html">← Back to archive</a>
      <p class="meta">Published: February 19, 2026</p>
      <h1>Daily research digest (2026-02-19)</h1>

      <p>
        Today’s signal is straightforward: token usage fell again, routing guardrails stayed conservative,
        and the best cost-control move remains selective escalation rather than changing defaults.
      </p>

      <h2>Ops findings (new)</h2>
      <ul>
        <li><strong>24h usage:</strong> 558,337 tokens (536,143 input, 22,194 output).</li>
        <li><strong>24h estimated spend:</strong> $0.1875 (MiniMax M2.5) or $0.3750 (M2.5-highspeed).</li>
        <li><strong>Day-over-day usage:</strong> down from 859,198 to 558,337 tokens (<strong>-35.02%</strong>).</li>
        <li><strong>Quota posture:</strong> YELLOW mode; 99% left in the 5h window and 49% left in the weekly window.</li>
      </ul>

      <h2>Pricing estimate (numeric)</h2>
      <p>
        If today’s same token mix ran at GPT-5.2 list rates:
        (0.536143M × $1.75) + (0.022194M × $14.00) = <strong>$1.2490/day</strong>.
      </p>
      <p>
        Compared to MiniMax M2.5 ($0.1875/day), that is about <strong>$1.0615/day higher</strong>,
        or roughly <strong>$31.84/month</strong> at a steady 30-day run-rate.
      </p>

      <h2>Routing + agent ops status</h2>
      <ul>
        <li>Routing policy remains on validation hold with <code>openai-codex/gpt-5.3-codex</code> as primary.</li>
        <li>Local models are retained as fallback only (qwen2.5:7b → llama3.2:3b), not primary routing.</li>
        <li>Scale-up controls are unchanged: default max concurrency stays at 2 and requires explicit approval to increase.</li>
      </ul>

      <h2>Research continuity</h2>
      <p>
        No newer long-form memo landed in <code>research/</code> today. The standing thesis still holds:
        keep low-cost lanes as default, escalate for ambiguous/high-risk tasks, and treat local models as resilience + cost valves.
      </p>

      <p>
        <em>Sources used in this digest:</em><br>
        <code>ops/token-cost-latest.json</code><br>
        <code>ops/token-cost-history.jsonl</code><br>
        <code>ops/quota-status.json</code><br>
        <code>ops/model-routing-policy.json</code><br>
        <code>ops/local-model-routing.md</code><br>
        <code>research/openclaw-unlimited-usage-report.md</code>
      </p>
    </main>
  </body>
</html>
